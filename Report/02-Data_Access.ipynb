{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Section Overview\n",
    "\n",
    "In this section, we will outline how the data necessary to run the regression models is accessed. we will be using the large dataset from \"Our World In Data\" and augmenting it with various indexes provided by The Oxford Coronavirus Government Response Tracker (OxCGRT) project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Data location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Dataset: https://github.com/owid/covid-19-data/tree/master/public/data\n",
    "\n",
    "This link will take you to a GitHub repository. Within the README.md file within this repository, there is an option to download the complete COVID-19 dataset. Please download the CSV version from the various file type options. \n",
    "\n",
    "\n",
    "Index Dataset: https://github.com/OxCGRT/covid-policy-tracker/blob/masterdocumentation/index_methodology.md\n",
    "\n",
    "This link will take you to another GitHub repository, the Oxford Coronavirus Government Response tracker project. In Particular, it will take you to the methodology for calculating indices. Under the Indices header, there is an option to download the OxCGRT_latest.csv file containing the necessary indices we shall use for this project. \n",
    "\n",
    "### Indices Outine\n",
    "\n",
    "Here, we will provide a brief description of the indices that\n",
    "\n",
    "[INSERT]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to continue, we need to combine the 2 datasets. In this section we will take the necessary steps to augment the datasets. Note, the augmented test and training datasets are already stored within the GitHub Repository under Train_and_Test_data however we will still show the steps needed to get to these final augmented datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset 1 (e.g., COVID-19 data from Our World in Data)\n",
    "\n",
    "owid_filepath = r'C:\\Users\\markm\\OneDrive\\Documents\\University\\Year 4\\dst\\data\\our_world_in_data_covid_data.csv' # Enter your file path for the our world in data dataset\n",
    "ox_index_filepath = r'C:\\Users\\markm\\OneDrive\\Documents\\University\\Year 4\\dst\\data\\OxCGRT_timeseries_all.csv' # Enter your file path for the Oxford Coronavirus government response tracker project dataset\n",
    "\n",
    "df_covid = pd.read_csv(owid_filepath)\n",
    "\n",
    "# Load Dataset 2 (e.g., Policy Tracker from OxCGRT)\n",
    "df_policy = pd.read_csv(ox_index_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the oxford index dataset, there are many columns that we don't require. For that reason, let us extract only the columns that we want to keep (these being those containing the various indices that we care about). Additionally, we only want to consider these indices on a national level. Certain countries (for example, Australia) have multiple rows as their state-level data is provided seperately. For that reason, we will filter out those rows and only include rows that contain data on a national jurisdiction level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ox_index_filepath = r'C:\\Users\\markm\\OneDrive\\Documents\\University\\Year 4\\dst\\data\\OxCGRT_timeseries_all.xlsx'\n",
    "\n",
    "df_stringency = pd.read_excel(ox_index_filepath, sheet_name='stringency_index_avg')\n",
    "df_containment_health = pd.read_excel(ox_index_filepath, sheet_name='containment_health_index_avg')\n",
    "df_government_response = pd.read_excel(ox_index_filepath, sheet_name='government_response_index_avg')\n",
    "df_economic_support = pd.read_excel(ox_index_filepath, sheet_name='economic_support_index')\n",
    "\n",
    "# List of the dataframes to process\n",
    "indexes = [df_stringency, df_containment_health, df_government_response, df_economic_support]\n",
    "filtered_indexes = []\n",
    "\n",
    "# Filter each dataframe and store the filtered version in 'filtered_indexes'\n",
    "for df in indexes:\n",
    "    filtered_df = df[df['jurisdiction'] == 'NAT_TOTAL']\n",
    "    filtered_indexes.append(filtered_df)\n",
    "\n",
    "# Assign the filtered dataframes back to their original variables (optional)\n",
    "df_stringency, df_containment_health, df_government_response, df_economic_support = filtered_indexes\n",
    "\n",
    "# Save the filtered dataframes as separate CSV files\n",
    "df_stringency.to_csv('stringency_index',index=False)\n",
    "df_containment_health.to_csv('containment_health_filtered.csv', index=False)\n",
    "df_government_response.to_csv('government_response_filtered.csv', index=False)\n",
    "df_economic_support.to_csv('economic_support_filtered.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country_code country_name region_code region_name jurisdiction  01Jan2020  \\\n",
      "0          ABW        Aruba         NaN         NaN    NAT_TOTAL        0.0   \n",
      "1          AFG  Afghanistan         NaN         NaN    NAT_TOTAL        0.0   \n",
      "2          AGO       Angola         NaN         NaN    NAT_TOTAL        0.0   \n",
      "3          ALB      Albania         NaN         NaN    NAT_TOTAL        0.0   \n",
      "4          AND      Andorra         NaN         NaN    NAT_TOTAL        0.0   \n",
      "\n",
      "   02Jan2020  03Jan2020  04Jan2020  05Jan2020  ...  11Feb2023  12Feb2023  \\\n",
      "0        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "1        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "2        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "3        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "4        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "\n",
      "   13Feb2023  14Feb2023  15Feb2023  16Feb2023  17Feb2023  18Feb2023  \\\n",
      "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "   19Feb2023  20Feb2023  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "\n",
      "[5 rows x 1152 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_containment_health.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have separated each index into its own dataframe. Our main problem is that unlike the OWID dataset where date is one column and there are multiple rows per country, the Oxford dataset has each date as is its own column and a singular row for a country. We need to reformat this so that it matches the format of the OWID dataset so that it can then be used in the model. To do this, we define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposer(dataset,index_name):\n",
    "    # Create a complete date range from January 5, 2020, to August 4, 2024 - this datarange matches that of each country in the OWID dataset\n",
    "    date_range = pd.date_range(start='05-Jan-2020', end='04-Aug-2024', freq='D')\n",
    "    complete_dates_df = pd.DataFrame(date_range, columns=['date'])  # Create a DataFrame with all dates\n",
    "\n",
    "# Create a list to store the complete DataFrames for each country\n",
    "    complete_dataframes = []\n",
    "\n",
    "# Iterate through each row in df_containment_health\n",
    "    for row_index in range(dataset.shape[0]):  # Iterate through rows\n",
    "        # Select the row you want to transpose (excluding first 9 columns) - we want to start on 05-Jan-2020\n",
    "        row_to_transpose = dataset.iloc[row_index][9:]  # Adjust index as needed\n",
    "        transposed_row = row_to_transpose.transpose()  # Transpose the row\n",
    "\n",
    "         # Convert the transposed row to a DataFrame\n",
    "        transposed_df = pd.DataFrame(transposed_row).reset_index()\n",
    "        transposed_df.columns = ['date', index_name+' ' 'Index']  # Rename columns for clarity\n",
    "\n",
    "         # Ensure the 'Date' column is in datetime format\n",
    "        transposed_df['date'] = pd.to_datetime(transposed_df['date'], format='%d%b%Y')\n",
    "\n",
    "        # Merge with the complete date range DataFrame\n",
    "        complete_df = complete_dates_df.merge(transposed_df, on='date', how='left')\n",
    "\n",
    "        # Add a new column for country code or name if needed\n",
    "        complete_df['iso_code'] = dataset.iloc[row_index, 0]  # Assuming the first column is the country code/name\n",
    "\n",
    "        # Append the complete DataFrame for this country to the list\n",
    "        complete_dataframes.append(complete_df)\n",
    "\n",
    "# Concatenate all the DataFrames in the list into a single DataFrame\n",
    "    final_df = pd.concat(complete_dataframes, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at what this function does to one of our dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Transposing\n",
      "  country_code country_name region_code region_name jurisdiction  01Jan2020  \\\n",
      "0          ABW        Aruba         NaN         NaN    NAT_TOTAL        0.0   \n",
      "1          AFG  Afghanistan         NaN         NaN    NAT_TOTAL        0.0   \n",
      "2          AGO       Angola         NaN         NaN    NAT_TOTAL        0.0   \n",
      "3          ALB      Albania         NaN         NaN    NAT_TOTAL        0.0   \n",
      "4          AND      Andorra         NaN         NaN    NAT_TOTAL        0.0   \n",
      "\n",
      "   02Jan2020  03Jan2020  04Jan2020  05Jan2020  ...  11Feb2023  12Feb2023  \\\n",
      "0        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "1        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "2        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "3        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "4        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "\n",
      "   13Feb2023  14Feb2023  15Feb2023  16Feb2023  17Feb2023  18Feb2023  \\\n",
      "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "   19Feb2023  20Feb2023  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "\n",
      "[5 rows x 1152 columns]\n",
      "\n",
      "After Transposing\n",
      "        date CH Index iso_code\n",
      "0 2020-01-05      0.0      ABW\n",
      "1 2020-01-06      0.0      ABW\n",
      "2 2020-01-07      0.0      ABW\n",
      "3 2020-01-08      0.0      ABW\n",
      "4 2020-01-09      0.0      ABW\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transposed_containment_health = transposer(df_containment_health, 'CH')\n",
    "\n",
    "print('Before Transposing')\n",
    "print(df_containment_health.head())\n",
    "print()\n",
    "print('After Transposing')\n",
    "print(transposed_containment_health.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now our data looks more similar to the data within the OWID dataset. Let's run this function on the remaining index datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_stringency = transposer(df_stringency,'Stringency')\n",
    "transposed_government_response = transposer(df_government_response, 'Gov Resp')\n",
    "transposed_economic_support = transposer(df_economic_support, 'Econ Sup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now combine all of these into a singular dataframe - namely, df_combined. Note that the iso_code and date columns will be used together as a composite primary key to differentiate between rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date iso_code Stringency Index CH Index Gov Resp Index  \\\n",
      "71     2020-03-16      ABW            11.11     9.52           8.33   \n",
      "72     2020-03-17      ABW            22.22    16.67          14.58   \n",
      "73     2020-03-18      ABW            33.33    26.19          22.92   \n",
      "74     2020-03-19      ABW            33.33    29.76          26.04   \n",
      "75     2020-03-20      ABW            33.33    29.76          26.04   \n",
      "...           ...      ...              ...      ...            ...   \n",
      "313033 2024-07-31      ZWE              NaN      NaN            NaN   \n",
      "313034 2024-08-01      ZWE              NaN      NaN            NaN   \n",
      "313035 2024-08-02      ZWE              NaN      NaN            NaN   \n",
      "313036 2024-08-03      ZWE              NaN      NaN            NaN   \n",
      "313037 2024-08-04      ZWE              NaN      NaN            NaN   \n",
      "\n",
      "       Econ Sup Index  \n",
      "71                0.0  \n",
      "72                0.0  \n",
      "73                0.0  \n",
      "74                0.0  \n",
      "75                0.0  \n",
      "...               ...  \n",
      "313033            NaN  \n",
      "313034            NaN  \n",
      "313035            NaN  \n",
      "313036            NaN  \n",
      "313037            NaN  \n",
      "\n",
      "[305821 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df_combined = transposed_stringency.join(transposed_containment_health, how='inner', lsuffix='', rsuffix='_containment')\n",
    "df_combined = df_combined.join(transposed_government_response, how='inner', lsuffix='_df_combined', rsuffix='_govresp')\n",
    "df_combined = df_combined.join(transposed_economic_support, how='inner', lsuffix='_df_combined', rsuffix='_econsup')\n",
    "# # Select only the desired columns\n",
    "df_combined = df_combined[['date', 'iso_code', 'Stringency Index', 'CH Index', 'Gov Resp Index', 'Econ Sup Index']]\n",
    "# Display the result - test output\n",
    "print(df_combined[df_combined['Stringency Index'] != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make sure our OWID dataset is in the appropriate format to combine with the Oxford dataset. First we will reformat the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['date'] = pd.to_datetime(df_covid['date'], format='%d/%m/%Y').dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2020-01-05\n",
      "1    2020-01-06\n",
      "2    2020-01-07\n",
      "3    2020-01-08\n",
      "4    2020-01-09\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_covid['date'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides this, the OWID dataset looks ready to be augmented with the indices we specified from the Oxford dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        date iso_code Stringency Index CH Index Gov Resp Index Econ Sup Index  \\\n",
      "0 2020-01-05      ABW              0.0      0.0            0.0            0.0   \n",
      "1 2020-01-06      ABW              0.0      0.0            0.0            0.0   \n",
      "2 2020-01-07      ABW              0.0      0.0            0.0            0.0   \n",
      "3 2020-01-08      ABW              0.0      0.0            0.0            0.0   \n",
      "4 2020-01-09      ABW              0.0      0.0            0.0            0.0   \n",
      "\n",
      "       continent location  total_cases  new_cases  ...  male_smokers  \\\n",
      "0  North America    Aruba          0.0        0.0  ...           NaN   \n",
      "1  North America    Aruba          0.0        0.0  ...           NaN   \n",
      "2  North America    Aruba          0.0        0.0  ...           NaN   \n",
      "3  North America    Aruba          0.0        0.0  ...           NaN   \n",
      "4  North America    Aruba          0.0        0.0  ...           NaN   \n",
      "\n",
      "   handwashing_facilities  hospital_beds_per_thousand  life_expectancy  \\\n",
      "0                     NaN                         NaN            76.29   \n",
      "1                     NaN                         NaN            76.29   \n",
      "2                     NaN                         NaN            76.29   \n",
      "3                     NaN                         NaN            76.29   \n",
      "4                     NaN                         NaN            76.29   \n",
      "\n",
      "   human_development_index  population  excess_mortality_cumulative_absolute  \\\n",
      "0                      NaN      106459                                   NaN   \n",
      "1                      NaN      106459                                   NaN   \n",
      "2                      NaN      106459                                   NaN   \n",
      "3                      NaN      106459                                   NaN   \n",
      "4                      NaN      106459                                   NaN   \n",
      "\n",
      "   excess_mortality_cumulative  excess_mortality  \\\n",
      "0                          NaN               NaN   \n",
      "1                          NaN               NaN   \n",
      "2                          NaN               NaN   \n",
      "3                          NaN               NaN   \n",
      "4                          NaN               NaN   \n",
      "\n",
      "   excess_mortality_cumulative_per_million  \n",
      "0                                      NaN  \n",
      "1                                      NaN  \n",
      "2                                      NaN  \n",
      "3                                      NaN  \n",
      "4                                      NaN  \n",
      "\n",
      "[5 rows x 71 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ensure the OWID dataset's date column is in datetime format\n",
    "df_covid['date'] = pd.to_datetime(df_covid['date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Ensure your other dataset's date column is also in datetime format (assuming it's named 'date')\n",
    "df_combined['date'] = pd.to_datetime(df_combined['date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Now perform the merge (assuming 'date' and 'iso_code' are the keys)\n",
    "merged_df = pd.merge(df_combined, df_covid, on=['iso_code', 'date'], how='inner')\n",
    "print()\n",
    "# Check the merged dataframe\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets have been merged without any immediate error messages however we are not yet finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our columns should be in an appropriate order, let us print them all out to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'iso_code', 'Stringency Index', 'CH Index', 'Gov Resp Index',\n",
      "       'Econ Sup Index', 'continent', 'location', 'total_cases', 'new_cases',\n",
      "       'new_cases_smoothed', 'total_deaths', 'new_deaths',\n",
      "       'new_deaths_smoothed', 'total_cases_per_million',\n",
      "       'new_cases_per_million', 'new_cases_smoothed_per_million',\n",
      "       'total_deaths_per_million', 'new_deaths_per_million',\n",
      "       'new_deaths_smoothed_per_million', 'reproduction_rate', 'icu_patients',\n",
      "       'icu_patients_per_million', 'hosp_patients',\n",
      "       'hosp_patients_per_million', 'weekly_icu_admissions',\n",
      "       'weekly_icu_admissions_per_million', 'weekly_hosp_admissions',\n",
      "       'weekly_hosp_admissions_per_million', 'total_tests', 'new_tests',\n",
      "       'total_tests_per_thousand', 'new_tests_per_thousand',\n",
      "       'new_tests_smoothed', 'new_tests_smoothed_per_thousand',\n",
      "       'positive_rate', 'tests_per_case', 'tests_units', 'total_vaccinations',\n",
      "       'people_vaccinated', 'people_fully_vaccinated', 'total_boosters',\n",
      "       'new_vaccinations', 'new_vaccinations_smoothed',\n",
      "       'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred',\n",
      "       'people_fully_vaccinated_per_hundred', 'total_boosters_per_hundred',\n",
      "       'new_vaccinations_smoothed_per_million',\n",
      "       'new_people_vaccinated_smoothed',\n",
      "       'new_people_vaccinated_smoothed_per_hundred', 'stringency_index',\n",
      "       'population_density', 'median_age', 'aged_65_older', 'aged_70_older',\n",
      "       'gdp_per_capita', 'extreme_poverty', 'cardiovasc_death_rate',\n",
      "       'diabetes_prevalence', 'female_smokers', 'male_smokers',\n",
      "       'handwashing_facilities', 'hospital_beds_per_thousand',\n",
      "       'life_expectancy', 'human_development_index', 'population',\n",
      "       'excess_mortality_cumulative_absolute', 'excess_mortality_cumulative',\n",
      "       'excess_mortality', 'excess_mortality_cumulative_per_million'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we do have 2 columns, Stringency Index and stringency index. Whilst this is not a large problem, it can be observed that they are not exactly numerically equivalenet. The below code displays row where these columns differ in their value. It is not an easy to decide on which column to use. On the one hand, it may be better to use the initial column since we have used it in previous analysis and it comes from our initial OWID dataset. However, the augmented column has more documentation and specific insight into how it was calculated and can therefore could provide a more reliable figure. For these reasons we will just take a mean average of these 2 columns and use this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where Column_A and Column_B differ:\n",
      "             date Stringency Index  stringency_index iso_code\n",
      "2326   2021-10-18            38.89             36.11      AFG\n",
      "2327   2021-10-19            38.89             36.11      AFG\n",
      "2328   2021-10-20            38.89             36.11      AFG\n",
      "2329   2021-10-21            38.89             36.11      AFG\n",
      "2330   2021-10-22            38.89             36.11      AFG\n",
      "...           ...              ...               ...      ...\n",
      "309280 2022-12-27             8.33             13.89      ZMB\n",
      "309281 2022-12-28             8.33             13.89      ZMB\n",
      "309282 2022-12-29             8.33             13.89      ZMB\n",
      "309283 2022-12-30             8.33             13.89      ZMB\n",
      "309284 2022-12-31             8.33             13.89      ZMB\n",
      "\n",
      "[20595 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "mask = merged_df['Stringency Index'] != merged_df['stringency_index']\n",
    "\n",
    "# Step 2: Filter the DataFrame using the mask\n",
    "differences = merged_df[mask][['date','Stringency Index','stringency_index','iso_code']].dropna()\n",
    "\n",
    "# test = differences[['stringency_index', 'Stringency Index']].mean(axis=1)\n",
    "# print(test)\n",
    "\n",
    "# Display the rows where the values differ\n",
    "print(\"Rows where Column_A and Column_B differ:\")\n",
    "\n",
    "print(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309280    11.11\n",
      "Name: mean_stringency, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate the mean and add it back to the original DataFrame\n",
    "merged_df['mean_stringency'] = merged_df[['Stringency Index', 'stringency_index']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Step 2: Apply the filtering conditions on the original DataFrame\n",
    "selected_row_mean_check = merged_df.loc[(merged_df['date'] == '2022-12-27') & (merged_df['iso_code'] == 'ZMB'), 'mean_stringency']\n",
    "\n",
    "# Display the result\n",
    "print(selected_row_mean_check)\n",
    "\n",
    "\n",
    "merged_df = merged_df.drop(columns=['Stringency Index'])\n",
    "merged_df = merged_df.drop(columns=['stringency_index'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above block takes the average of the two columns for some row. We can check it is working by computing the average by hand for some row and checking it is the same provided above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us move the continent and location columns to the front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>mean_stringency</th>\n",
       "      <th>CH Index</th>\n",
       "      <th>Gov Resp Index</th>\n",
       "      <th>Econ Sup Index</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>...</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>population</th>\n",
       "      <th>excess_mortality_cumulative_absolute</th>\n",
       "      <th>excess_mortality_cumulative</th>\n",
       "      <th>excess_mortality</th>\n",
       "      <th>excess_mortality_cumulative_per_million</th>\n",
       "      <th>mean_stringency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date iso_code      continent location  mean_stringency CH Index  \\\n",
       "0 2020-01-05      ABW  North America    Aruba              0.0      0.0   \n",
       "1 2020-01-06      ABW  North America    Aruba              0.0      0.0   \n",
       "2 2020-01-07      ABW  North America    Aruba              0.0      0.0   \n",
       "3 2020-01-08      ABW  North America    Aruba              0.0      0.0   \n",
       "4 2020-01-09      ABW  North America    Aruba              0.0      0.0   \n",
       "\n",
       "  Gov Resp Index Econ Sup Index  total_cases  new_cases  ...  \\\n",
       "0            0.0            0.0          0.0        0.0  ...   \n",
       "1            0.0            0.0          0.0        0.0  ...   \n",
       "2            0.0            0.0          0.0        0.0  ...   \n",
       "3            0.0            0.0          0.0        0.0  ...   \n",
       "4            0.0            0.0          0.0        0.0  ...   \n",
       "\n",
       "   handwashing_facilities  hospital_beds_per_thousand  life_expectancy  \\\n",
       "0                     NaN                         NaN            76.29   \n",
       "1                     NaN                         NaN            76.29   \n",
       "2                     NaN                         NaN            76.29   \n",
       "3                     NaN                         NaN            76.29   \n",
       "4                     NaN                         NaN            76.29   \n",
       "\n",
       "   human_development_index  population  excess_mortality_cumulative_absolute  \\\n",
       "0                      NaN      106459                                   NaN   \n",
       "1                      NaN      106459                                   NaN   \n",
       "2                      NaN      106459                                   NaN   \n",
       "3                      NaN      106459                                   NaN   \n",
       "4                      NaN      106459                                   NaN   \n",
       "\n",
       "   excess_mortality_cumulative  excess_mortality  \\\n",
       "0                          NaN               NaN   \n",
       "1                          NaN               NaN   \n",
       "2                          NaN               NaN   \n",
       "3                          NaN               NaN   \n",
       "4                          NaN               NaN   \n",
       "\n",
       "   excess_mortality_cumulative_per_million  mean_stringency  \n",
       "0                                      NaN              0.0  \n",
       "1                                      NaN              0.0  \n",
       "2                                      NaN              0.0  \n",
       "3                                      NaN              0.0  \n",
       "4                                      NaN              0.0  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns = list(merged_df.columns)\n",
    "# Move 'continent' and 'location' next to 'date' and 'iso_code'\n",
    "new_column_order = ['date', 'iso_code', 'continent', 'location', 'mean_stringency'] + [col for col in all_columns if col not in ['date', 'iso_code', 'continent', 'location']]\n",
    "merged_df = merged_df[new_column_order]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to add an additional column to the dataset. this column will be called days_since. This column will effectively represent the date column in our future regression models however these models do not interpret timestamp objects and we will get an error that looks like this:\n",
    "\n",
    "TypeError: float() argument must be a string or a real number, not 'Timestamp'\n",
    "\n",
    "Hence this new column is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['days_since'] = merged_df.groupby('iso_code')['date'].transform(lambda x: (x - x.min()).dt.days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column will start at 0 (representing the first day of recording, the 5th of January 2020) and end at 1673 (representing the last day of recording, the 4th of August 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'iso_code', 'continent', 'location', 'mean_stringency',\n",
      "       'CH Index', 'Gov Resp Index', 'Econ Sup Index', 'total_cases',\n",
      "       'new_cases', 'new_cases_smoothed', 'total_deaths', 'new_deaths',\n",
      "       'new_deaths_smoothed', 'total_cases_per_million',\n",
      "       'new_cases_per_million', 'new_cases_smoothed_per_million',\n",
      "       'total_deaths_per_million', 'new_deaths_per_million',\n",
      "       'new_deaths_smoothed_per_million', 'reproduction_rate', 'icu_patients',\n",
      "       'icu_patients_per_million', 'hosp_patients',\n",
      "       'hosp_patients_per_million', 'weekly_icu_admissions',\n",
      "       'weekly_icu_admissions_per_million', 'weekly_hosp_admissions',\n",
      "       'weekly_hosp_admissions_per_million', 'total_tests', 'new_tests',\n",
      "       'total_tests_per_thousand', 'new_tests_per_thousand',\n",
      "       'new_tests_smoothed', 'new_tests_smoothed_per_thousand',\n",
      "       'positive_rate', 'tests_per_case', 'tests_units', 'total_vaccinations',\n",
      "       'people_vaccinated', 'people_fully_vaccinated', 'total_boosters',\n",
      "       'new_vaccinations', 'new_vaccinations_smoothed',\n",
      "       'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred',\n",
      "       'people_fully_vaccinated_per_hundred', 'total_boosters_per_hundred',\n",
      "       'new_vaccinations_smoothed_per_million',\n",
      "       'new_people_vaccinated_smoothed',\n",
      "       'new_people_vaccinated_smoothed_per_hundred', 'population_density',\n",
      "       'median_age', 'aged_65_older', 'aged_70_older', 'gdp_per_capita',\n",
      "       'extreme_poverty', 'cardiovasc_death_rate', 'diabetes_prevalence',\n",
      "       'female_smokers', 'male_smokers', 'handwashing_facilities',\n",
      "       'hospital_beds_per_thousand', 'life_expectancy',\n",
      "       'human_development_index', 'population',\n",
      "       'excess_mortality_cumulative_absolute', 'excess_mortality_cumulative',\n",
      "       'excess_mortality', 'excess_mortality_cumulative_per_million',\n",
      "       'mean_stringency', 'days_since'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Splitting our Data into Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to split our data up into test and training data. Due to the nature of this particular project where we are aiming to predict the reproduction rate for a particular country over the entire period of time, it makes intuitive sense to due a train/test split on a country level i.e. we aren't going to design a split such that some of a country's data is in training and some of it is in testing. \n",
    "\n",
    "Let us first see how many countries there are in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABW' 'AFG' 'AGO' 'ALB' 'AND' 'ARE' 'ARG' 'AUS' 'AUT' 'AZE' 'BDI' 'BEL'\n",
      " 'BEN' 'BFA' 'BGD' 'BGR' 'BHR' 'BHS' 'BIH' 'BLR' 'BLZ' 'BMU' 'BOL' 'BRA'\n",
      " 'BRB' 'BRN' 'BTN' 'BWA' 'CAF' 'CAN' 'CHE' 'CHL' 'CHN' 'CIV' 'CMR' 'COD'\n",
      " 'COG' 'COL' 'COM' 'CPV' 'CRI' 'CUB' 'CYP' 'CZE' 'DEU' 'DJI' 'DMA' 'DNK'\n",
      " 'DOM' 'DZA' 'ECU' 'EGY' 'ERI' 'ESP' 'EST' 'ETH' 'FIN' 'FJI' 'FRA' 'FRO'\n",
      " 'GAB' 'GBR' 'GEO' 'GHA' 'GIN' 'GMB' 'GRC' 'GRD' 'GRL' 'GTM' 'GUM' 'GUY'\n",
      " 'HKG' 'HND' 'HRV' 'HTI' 'HUN' 'IDN' 'IND' 'IRL' 'IRN' 'IRQ' 'ISL' 'ISR'\n",
      " 'ITA' 'JAM' 'JOR' 'JPN' 'KAZ' 'KEN' 'KGZ' 'KHM' 'KIR' 'KOR' 'KWT' 'LAO'\n",
      " 'LBN' 'LBR' 'LBY' 'LIE' 'LKA' 'LSO' 'LTU' 'LUX' 'LVA' 'MAC' 'MAR' 'MCO'\n",
      " 'MDA' 'MDG' 'MEX' 'MLI' 'MLT' 'MMR' 'MNG' 'MOZ' 'MRT' 'MUS' 'MWI' 'MYS'\n",
      " 'NAM' 'NER' 'NGA' 'NIC' 'NLD' 'NOR' 'NPL' 'NZL' 'OMN' 'PAK' 'PAN' 'PER'\n",
      " 'PHL' 'PNG' 'POL' 'PRI' 'PRT' 'PRY' 'PSE' 'QAT' 'ROU' 'RUS' 'RWA' 'SAU'\n",
      " 'SDN' 'SEN' 'SGP' 'SLB' 'SLE' 'SLV' 'SMR' 'SOM' 'SRB' 'SSD' 'SUR' 'SVK'\n",
      " 'SVN' 'SWE' 'SWZ' 'SYC' 'SYR' 'TCD' 'TGO' 'THA' 'TJK' 'TKM' 'TLS' 'TON'\n",
      " 'TTO' 'TUN' 'TUR' 'TWN' 'TZA' 'UGA' 'UKR' 'URY' 'USA' 'UZB' 'VEN' 'VIR'\n",
      " 'VNM' 'VUT' 'YEM' 'ZAF' 'ZMB' 'ZWE']\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['iso_code'].unique())\n",
    "print(len(merged_df['iso_code'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to have roughly an 80/20 split of training and testing. However, there are 186 countries so in this particular case, we shall randomely select 155 countries to be trained with and the remaining 31 to be tested with. This equates roughly to a 83/17 split of training and testing.\n",
    "\n",
    "To select a random sample of 155 elements from a set of 186 unique ISO codes in a Pandas DataFrame, we can use the sample() method. This method allows you to specify the number of samples you want to draw and will randomly select from the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BFA', 'BLR', 'TUN', 'PRI', 'GUM', 'AZE', 'RUS', 'LTU', 'ERI', 'AGO', 'NLD', 'SUR', 'KAZ', 'BMU', 'MAR', 'TCD', 'MRT', 'BHS', 'BLZ', 'DJI', 'UZB', 'MNG', 'TUR', 'MDA', 'RWA', 'CYP', 'LSO', 'ARG', 'BOL', 'GAB', 'CRI', 'SVK', 'ISR', 'SVN', 'SLV', 'BTN', 'VUT', 'MMR', 'KHM', 'SWZ', 'COL', 'TJK', 'TTO', 'PSE', 'BRA', 'SWE', 'SRB', 'DEU', 'ROU', 'VEN', 'NOR', 'LVA', 'AUS', 'TWN', 'JPN', 'VNM', 'SMR', 'EST', 'JAM', 'USA', 'GEO', 'HTI', 'BGD', 'SAU', 'ZWE', 'ABW', 'SEN', 'MOZ', 'MLI', 'SSD', 'GRL', 'BDI', 'SYR', 'NZL', 'URY', 'LAO', 'CUB', 'KWT', 'SGP', 'DMA', 'COM', 'ALB', 'CAN', 'HND', 'LBR', 'COG', 'ETH', 'IDN', 'VIR', 'BEL', 'PAN', 'DZA', 'MAC', 'IRL', 'GMB', 'PAK', 'GRD', 'IRQ', 'KEN', 'KOR', 'HRV', 'JOR', 'QAT', 'CHL', 'CHN', 'PRT', 'BWA', 'SOM', 'EGY', 'AFG', 'LIE', 'BHR', 'FJI', 'SLB', 'TKM', 'CMR', 'ZMB', 'LBN', 'MUS', 'NIC', 'TLS', 'CZE', 'TGO', 'CPV', 'DNK', 'IND', 'KIR', 'PER', 'GIN', 'MCO', 'NPL', 'BRN', 'BRB', 'LUX', 'FRA', 'AUT', 'CIV', 'GUY', 'NER', 'GRC', 'GHA', 'SYC', 'UKR', 'SDN', 'CHE', 'HUN', 'PHL', 'MWI', 'GTM', 'TON', 'OMN', 'MDG', 'PRY', 'CAF', 'KGZ']\n",
      "155\n",
      "['ISL', 'LBY', 'NGA', 'PNG', 'LKA', 'DOM', 'IRN', 'TZA', 'GBR', 'BIH', 'COD', 'BGR', 'AND', 'ECU', 'FRO', 'FIN', 'ESP', 'SLE', 'MYS', 'MEX', 'POL', 'ARE', 'NAM', 'MLT', 'HKG', 'ITA', 'THA', 'YEM', 'BEN', 'UGA', 'ZAF']\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(21) # for replicability, let's introduce a random seed\n",
    "unique_iso_codes = merged_df['iso_code'].unique()\n",
    "random_sample = np.random.choice(unique_iso_codes, size=155, replace=False)\n",
    "# Convert to list for easier handling if needed\n",
    "random_sample_list = random_sample.tolist()\n",
    "# Display the randomly selected iso codes\n",
    "print(random_sample_list)\n",
    "print(len(random_sample_list))\n",
    "\n",
    "training_iso_codes = random_sample_list\n",
    "testing_iso_codes = list(set(unique_iso_codes) - set(training_iso_codes))\n",
    "\n",
    "print(testing_iso_codes)\n",
    "print(len(testing_iso_codes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we've got our training and testing split. Now let's split the dataset itself into our training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'iso_code', 'continent', 'location', 'mean_stringency',\n",
      "       'CH Index', 'Gov Resp Index', 'Econ Sup Index', 'total_cases',\n",
      "       'new_cases', 'new_cases_smoothed', 'total_deaths', 'new_deaths',\n",
      "       'new_deaths_smoothed', 'total_cases_per_million',\n",
      "       'new_cases_per_million', 'new_cases_smoothed_per_million',\n",
      "       'total_deaths_per_million', 'new_deaths_per_million',\n",
      "       'new_deaths_smoothed_per_million', 'reproduction_rate', 'icu_patients',\n",
      "       'icu_patients_per_million', 'hosp_patients',\n",
      "       'hosp_patients_per_million', 'weekly_icu_admissions',\n",
      "       'weekly_icu_admissions_per_million', 'weekly_hosp_admissions',\n",
      "       'weekly_hosp_admissions_per_million', 'total_tests', 'new_tests',\n",
      "       'total_tests_per_thousand', 'new_tests_per_thousand',\n",
      "       'new_tests_smoothed', 'new_tests_smoothed_per_thousand',\n",
      "       'positive_rate', 'tests_per_case', 'tests_units', 'total_vaccinations',\n",
      "       'people_vaccinated', 'people_fully_vaccinated', 'total_boosters',\n",
      "       'new_vaccinations', 'new_vaccinations_smoothed',\n",
      "       'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred',\n",
      "       'people_fully_vaccinated_per_hundred', 'total_boosters_per_hundred',\n",
      "       'new_vaccinations_smoothed_per_million',\n",
      "       'new_people_vaccinated_smoothed',\n",
      "       'new_people_vaccinated_smoothed_per_hundred', 'population_density',\n",
      "       'median_age', 'aged_65_older', 'aged_70_older', 'gdp_per_capita',\n",
      "       'extreme_poverty', 'cardiovasc_death_rate', 'diabetes_prevalence',\n",
      "       'female_smokers', 'male_smokers', 'handwashing_facilities',\n",
      "       'hospital_beds_per_thousand', 'life_expectancy',\n",
      "       'human_development_index', 'population',\n",
      "       'excess_mortality_cumulative_absolute', 'excess_mortality_cumulative',\n",
      "       'excess_mortality', 'excess_mortality_cumulative_per_million',\n",
      "       'mean_stringency', 'days_since'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_data = merged_df[merged_df['iso_code'].isin(training_iso_codes)]\n",
    "test_data = merged_df[merged_df['iso_code'].isin(testing_iso_codes)]\n",
    "\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have succefully split our data into training data and test date, augmented with the indices data from the Oxford Dataset. the following code cell takes approximately 10 minutes to run. For that reason, instead of running it yourself, the output train and test xlsx files can be accessed directly via the github in Train_and_Test_data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_excel('train.xlsx', index=False)\n",
    "test_data.to_excel('test.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
