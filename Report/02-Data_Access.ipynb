{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Section Overview\n",
    "\n",
    "In this section, we will outline how the data necessary to run the regression models is accessed. we will be using the large dataset from \"Our World In Data\" and augmenting it with various indexes provided by The Oxford Coronavirus Government Response Tracker (OxCGRT) project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Data location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Dataset: https://github.com/owid/covid-19-data/tree/master/public/data\n",
    "\n",
    "This link will take you to a GitHub repository. Within the README.md file within this repository, there is an option to download the complete COVID-19 dataset. Please download the CSV version from the various file type options. \n",
    "\n",
    "\n",
    "Index Dataset: https://github.com/OxCGRT/covid-policy-tracker/blob/masterdocumentation/index_methodology.md\n",
    "\n",
    "This link will take you to another GitHub repository, the Oxford Coronavirus Government Response tracker project. In Particular, it will take you to the methodology for calculating indices. Under the Indices header, there is an option to download the OxCGRT_latest.csv file containing the necessary indices we shall use for this project. \n",
    "\n",
    "### Indices Outine\n",
    "\n",
    "Here, we will provide a brief description of the indices that\n",
    "\n",
    "[INSERT]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to continue, we need to combine the 2 datasets. In this section we will take the necessary steps to augment the datasets. Note, the augmented test and training datasets are already stored within the GitHub Repository under Train_and_Test_data however we will still show the steps needed to get to these final augmented datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset 1 (e.g., COVID-19 data from Our World in Data)\n",
    "\n",
    "owid_filepath = r'C:\\Users\\markm\\OneDrive\\Documents\\University\\Year 4\\dst\\data\\our_world_in_data_covid_data.csv' # Enter your file path for the our world in data dataset\n",
    "ox_index_filepath = r'C:\\Users\\markm\\OneDrive\\Documents\\University\\Year 4\\dst\\data\\OxCGRT_timeseries_all.csv' # Enter your file path for the Oxford Coronavirus government response tracker project dataset\n",
    "\n",
    "df_covid = pd.read_csv(owid_filepath)\n",
    "\n",
    "# Load Dataset 2 (e.g., Policy Tracker from OxCGRT)\n",
    "df_policy = pd.read_csv(ox_index_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the oxford index dataset, there are many columns that we don't require. For that reason, let us extract only the columns that we want to keep (these being those containing the various indices that we care about). Additionally, we only want to consider these indices on a national level. Certain countries (for example, Australia) have multiple rows as their state-level data is provided seperately. For that reason, we will filter out those rows and only include rows that contain data on a national jurisdiction level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ox_index_filepath = r'C:\\Users\\markm\\OneDrive\\Documents\\University\\Year 4\\dst\\data\\OxCGRT_timeseries_all.xlsx'\n",
    "\n",
    "df_stringency = pd.read_excel(ox_index_filepath, sheet_name='stringency_index_avg')\n",
    "df_containment_health = pd.read_excel(ox_index_filepath, sheet_name='containment_health_index_avg')\n",
    "df_government_response = pd.read_excel(ox_index_filepath, sheet_name='government_response_index_avg')\n",
    "df_economic_support = pd.read_excel(ox_index_filepath, sheet_name='economic_support_index')\n",
    "\n",
    "# List of the dataframes to process\n",
    "indexes = [df_stringency, df_containment_health, df_government_response, df_economic_support]\n",
    "filtered_indexes = []\n",
    "\n",
    "# Filter each dataframe and store the filtered version in 'filtered_indexes'\n",
    "for df in indexes:\n",
    "    filtered_df = df[df['jurisdiction'] == 'NAT_TOTAL']\n",
    "    filtered_indexes.append(filtered_df)\n",
    "\n",
    "# Assign the filtered dataframes back to their original variables (optional)\n",
    "df_stringency, df_containment_health, df_government_response, df_economic_support = filtered_indexes\n",
    "\n",
    "# Save the filtered dataframes as separate CSV files\n",
    "df_stringency.to_csv('stringency_index',index=False)\n",
    "df_containment_health.to_csv('containment_health_filtered.csv', index=False)\n",
    "df_government_response.to_csv('government_response_filtered.csv', index=False)\n",
    "df_economic_support.to_csv('economic_support_filtered.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country_code country_name region_code region_name jurisdiction  01Jan2020  \\\n",
      "0          ABW        Aruba         NaN         NaN    NAT_TOTAL        0.0   \n",
      "1          AFG  Afghanistan         NaN         NaN    NAT_TOTAL        0.0   \n",
      "2          AGO       Angola         NaN         NaN    NAT_TOTAL        0.0   \n",
      "3          ALB      Albania         NaN         NaN    NAT_TOTAL        0.0   \n",
      "4          AND      Andorra         NaN         NaN    NAT_TOTAL        0.0   \n",
      "\n",
      "   02Jan2020  03Jan2020  04Jan2020  05Jan2020  ...  11Feb2023  12Feb2023  \\\n",
      "0        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "1        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "2        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "3        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "4        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "\n",
      "   13Feb2023  14Feb2023  15Feb2023  16Feb2023  17Feb2023  18Feb2023  \\\n",
      "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "   19Feb2023  20Feb2023  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "\n",
      "[5 rows x 1152 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_containment_health.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have separated each index into its own dataframe. Our main problem is that unlike the OWID dataset where date is one column and there are multiple rows per country, the Oxford dataset has each date as is its own column and a singular row for a country. We need to reformat this so that it matches the format of the OWID dataset so that it can then be used in the model. To do this, we define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposer(dataset,index_name):\n",
    "    # Create a complete date range from January 5, 2020, to August 4, 2024 - this datarange matches that of each country in the OWID dataset\n",
    "    date_range = pd.date_range(start='05-Jan-2020', end='04-Aug-2024', freq='D')\n",
    "    complete_dates_df = pd.DataFrame(date_range, columns=['date'])  # Create a DataFrame with all dates\n",
    "\n",
    "# Create a list to store the complete DataFrames for each country\n",
    "    complete_dataframes = []\n",
    "\n",
    "# Iterate through each row in df_containment_health\n",
    "    for row_index in range(dataset.shape[0]):  # Iterate through rows\n",
    "        # Select the row you want to transpose (excluding first 9 columns) - we want to start on 05-Jan-2020\n",
    "        row_to_transpose = dataset.iloc[row_index][9:]  # Adjust index as needed\n",
    "        transposed_row = row_to_transpose.transpose()  # Transpose the row\n",
    "\n",
    "         # Convert the transposed row to a DataFrame\n",
    "        transposed_df = pd.DataFrame(transposed_row).reset_index()\n",
    "        transposed_df.columns = ['date', index_name+' ' 'Index']  # Rename columns for clarity\n",
    "\n",
    "         # Ensure the 'Date' column is in datetime format\n",
    "        transposed_df['date'] = pd.to_datetime(transposed_df['date'], format='%d%b%Y')\n",
    "\n",
    "        # Merge with the complete date range DataFrame\n",
    "        complete_df = complete_dates_df.merge(transposed_df, on='date', how='left')\n",
    "\n",
    "        # Add a new column for country code or name if needed\n",
    "        complete_df['iso_code'] = dataset.iloc[row_index, 0]  # Assuming the first column is the country code/name\n",
    "\n",
    "        # Append the complete DataFrame for this country to the list\n",
    "        complete_dataframes.append(complete_df)\n",
    "\n",
    "# Concatenate all the DataFrames in the list into a single DataFrame\n",
    "    final_df = pd.concat(complete_dataframes, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at what this function does to one of our dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Transposing\n",
      "  country_code country_name region_code region_name jurisdiction  01Jan2020  \\\n",
      "0          ABW        Aruba         NaN         NaN    NAT_TOTAL        0.0   \n",
      "1          AFG  Afghanistan         NaN         NaN    NAT_TOTAL        0.0   \n",
      "2          AGO       Angola         NaN         NaN    NAT_TOTAL        0.0   \n",
      "3          ALB      Albania         NaN         NaN    NAT_TOTAL        0.0   \n",
      "4          AND      Andorra         NaN         NaN    NAT_TOTAL        0.0   \n",
      "\n",
      "   02Jan2020  03Jan2020  04Jan2020  05Jan2020  ...  11Feb2023  12Feb2023  \\\n",
      "0        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "1        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "2        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "3        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "4        0.0        0.0        0.0        0.0  ...        NaN        NaN   \n",
      "\n",
      "   13Feb2023  14Feb2023  15Feb2023  16Feb2023  17Feb2023  18Feb2023  \\\n",
      "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "   19Feb2023  20Feb2023  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "\n",
      "[5 rows x 1152 columns]\n",
      "\n",
      "After Transposing\n",
      "        date CH Index iso_code\n",
      "0 2020-01-05      0.0      ABW\n",
      "1 2020-01-06      0.0      ABW\n",
      "2 2020-01-07      0.0      ABW\n",
      "3 2020-01-08      0.0      ABW\n",
      "4 2020-01-09      0.0      ABW\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transposed_containment_health = transposer(df_containment_health, 'CH')\n",
    "\n",
    "print('Before Transposing')\n",
    "print(df_containment_health.head())\n",
    "print()\n",
    "print('After Transposing')\n",
    "print(transposed_containment_health.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now our data looks more similar to the data within the OWID dataset. Let's run this function on the remaining index datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_stringency = transposer(df_stringency,'Stringency')\n",
    "transposed_government_response = transposer(df_government_response, 'Gov Resp')\n",
    "transposed_economic_support = transposer(df_economic_support, 'Econ Sup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now combine all of these into a singular dataframe - namely, df_combined. Note that the iso_code and date columns will be used together as a composite primary key to differentiate between rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date iso_code Stringency Index CH Index Gov Resp Index  \\\n",
      "71     2020-03-16      ABW            11.11     9.52           8.33   \n",
      "72     2020-03-17      ABW            22.22    16.67          14.58   \n",
      "73     2020-03-18      ABW            33.33    26.19          22.92   \n",
      "74     2020-03-19      ABW            33.33    29.76          26.04   \n",
      "75     2020-03-20      ABW            33.33    29.76          26.04   \n",
      "...           ...      ...              ...      ...            ...   \n",
      "313033 2024-07-31      ZWE              NaN      NaN            NaN   \n",
      "313034 2024-08-01      ZWE              NaN      NaN            NaN   \n",
      "313035 2024-08-02      ZWE              NaN      NaN            NaN   \n",
      "313036 2024-08-03      ZWE              NaN      NaN            NaN   \n",
      "313037 2024-08-04      ZWE              NaN      NaN            NaN   \n",
      "\n",
      "       Econ Sup Index  \n",
      "71                0.0  \n",
      "72                0.0  \n",
      "73                0.0  \n",
      "74                0.0  \n",
      "75                0.0  \n",
      "...               ...  \n",
      "313033            NaN  \n",
      "313034            NaN  \n",
      "313035            NaN  \n",
      "313036            NaN  \n",
      "313037            NaN  \n",
      "\n",
      "[305821 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df_combined = transposed_stringency.join(transposed_containment_health, how='inner', lsuffix='', rsuffix='_containment')\n",
    "df_combined = df_combined.join(transposed_government_response, how='inner', lsuffix='_df_combined', rsuffix='_govresp')\n",
    "df_combined = df_combined.join(transposed_economic_support, how='inner', lsuffix='_df_combined', rsuffix='_econsup')\n",
    "# # Select only the desired columns\n",
    "df_combined = df_combined[['date', 'iso_code', 'Stringency Index', 'CH Index', 'Gov Resp Index', 'Econ Sup Index']]\n",
    "# Display the result - test output\n",
    "print(df_combined[df_combined['Stringency Index'] != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make sure our OWID dataset is in the appropriate format to combine with the Oxford dataset. First we will reformat the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['date'] = pd.to_datetime(df_covid['date'], format='%d/%m/%Y').dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2020-01-05\n",
      "1    2020-01-06\n",
      "2    2020-01-07\n",
      "3    2020-01-08\n",
      "4    2020-01-09\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_covid['date'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides this, the OWID dataset looks ready to be augmented with the indices we specified from the Oxford dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        date iso_code Stringency Index CH Index Gov Resp Index Econ Sup Index  \\\n",
      "0 2020-01-05      ABW              0.0      0.0            0.0            0.0   \n",
      "1 2020-01-06      ABW              0.0      0.0            0.0            0.0   \n",
      "2 2020-01-07      ABW              0.0      0.0            0.0            0.0   \n",
      "3 2020-01-08      ABW              0.0      0.0            0.0            0.0   \n",
      "4 2020-01-09      ABW              0.0      0.0            0.0            0.0   \n",
      "\n",
      "       continent location  total_cases  new_cases  ...  male_smokers  \\\n",
      "0  North America    Aruba          0.0        0.0  ...           NaN   \n",
      "1  North America    Aruba          0.0        0.0  ...           NaN   \n",
      "2  North America    Aruba          0.0        0.0  ...           NaN   \n",
      "3  North America    Aruba          0.0        0.0  ...           NaN   \n",
      "4  North America    Aruba          0.0        0.0  ...           NaN   \n",
      "\n",
      "   handwashing_facilities  hospital_beds_per_thousand  life_expectancy  \\\n",
      "0                     NaN                         NaN            76.29   \n",
      "1                     NaN                         NaN            76.29   \n",
      "2                     NaN                         NaN            76.29   \n",
      "3                     NaN                         NaN            76.29   \n",
      "4                     NaN                         NaN            76.29   \n",
      "\n",
      "   human_development_index  population  excess_mortality_cumulative_absolute  \\\n",
      "0                      NaN      106459                                   NaN   \n",
      "1                      NaN      106459                                   NaN   \n",
      "2                      NaN      106459                                   NaN   \n",
      "3                      NaN      106459                                   NaN   \n",
      "4                      NaN      106459                                   NaN   \n",
      "\n",
      "   excess_mortality_cumulative  excess_mortality  \\\n",
      "0                          NaN               NaN   \n",
      "1                          NaN               NaN   \n",
      "2                          NaN               NaN   \n",
      "3                          NaN               NaN   \n",
      "4                          NaN               NaN   \n",
      "\n",
      "   excess_mortality_cumulative_per_million  \n",
      "0                                      NaN  \n",
      "1                                      NaN  \n",
      "2                                      NaN  \n",
      "3                                      NaN  \n",
      "4                                      NaN  \n",
      "\n",
      "[5 rows x 71 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ensure the OWID dataset's date column is in datetime format\n",
    "df_covid['date'] = pd.to_datetime(df_covid['date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Ensure your other dataset's date column is also in datetime format (assuming it's named 'date')\n",
    "df_combined['date'] = pd.to_datetime(df_combined['date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Now perform the merge (assuming 'date' and 'iso_code' are the keys)\n",
    "merged_df = pd.merge(df_combined, df_covid, on=['iso_code', 'date'], how='inner')\n",
    "print()\n",
    "# Check the merged dataframe\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets have been merged without any immediate error messages however we are not yet finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our columns should be in an appropriate order, let us print them all out to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'iso_code', 'Stringency Index', 'CH Index', 'Gov Resp Index',\n",
      "       'Econ Sup Index', 'continent', 'location', 'total_cases', 'new_cases',\n",
      "       'new_cases_smoothed', 'total_deaths', 'new_deaths',\n",
      "       'new_deaths_smoothed', 'total_cases_per_million',\n",
      "       'new_cases_per_million', 'new_cases_smoothed_per_million',\n",
      "       'total_deaths_per_million', 'new_deaths_per_million',\n",
      "       'new_deaths_smoothed_per_million', 'reproduction_rate', 'icu_patients',\n",
      "       'icu_patients_per_million', 'hosp_patients',\n",
      "       'hosp_patients_per_million', 'weekly_icu_admissions',\n",
      "       'weekly_icu_admissions_per_million', 'weekly_hosp_admissions',\n",
      "       'weekly_hosp_admissions_per_million', 'total_tests', 'new_tests',\n",
      "       'total_tests_per_thousand', 'new_tests_per_thousand',\n",
      "       'new_tests_smoothed', 'new_tests_smoothed_per_thousand',\n",
      "       'positive_rate', 'tests_per_case', 'tests_units', 'total_vaccinations',\n",
      "       'people_vaccinated', 'people_fully_vaccinated', 'total_boosters',\n",
      "       'new_vaccinations', 'new_vaccinations_smoothed',\n",
      "       'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred',\n",
      "       'people_fully_vaccinated_per_hundred', 'total_boosters_per_hundred',\n",
      "       'new_vaccinations_smoothed_per_million',\n",
      "       'new_people_vaccinated_smoothed',\n",
      "       'new_people_vaccinated_smoothed_per_hundred', 'stringency_index',\n",
      "       'population_density', 'median_age', 'aged_65_older', 'aged_70_older',\n",
      "       'gdp_per_capita', 'extreme_poverty', 'cardiovasc_death_rate',\n",
      "       'diabetes_prevalence', 'female_smokers', 'male_smokers',\n",
      "       'handwashing_facilities', 'hospital_beds_per_thousand',\n",
      "       'life_expectancy', 'human_development_index', 'population',\n",
      "       'excess_mortality_cumulative_absolute', 'excess_mortality_cumulative',\n",
      "       'excess_mortality', 'excess_mortality_cumulative_per_million'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we do have 2 columns, Stringency Index and stringency index. Whilst this is not a large problem, it can be observed that they are not exactly numerically equivalenet. The below code displays row where these columns differ in their value. It is not an easy to decide on which column to use. On the one hand, it may be better to use the initial column since we have used it in previous analysis and it comes from our initial OWID dataset. However, the augmented column has more documentation and specific insight into how it was calculated and can therefore could provide a more reliable figure. For these reasons we will just take a mean average of these 2 columns and use this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where Column_A and Column_B differ:\n",
      "             date Stringency Index  stringency_index iso_code\n",
      "2326   2021-10-18            38.89             36.11      AFG\n",
      "2327   2021-10-19            38.89             36.11      AFG\n",
      "2328   2021-10-20            38.89             36.11      AFG\n",
      "2329   2021-10-21            38.89             36.11      AFG\n",
      "2330   2021-10-22            38.89             36.11      AFG\n",
      "...           ...              ...               ...      ...\n",
      "309280 2022-12-27             8.33             13.89      ZMB\n",
      "309281 2022-12-28             8.33             13.89      ZMB\n",
      "309282 2022-12-29             8.33             13.89      ZMB\n",
      "309283 2022-12-30             8.33             13.89      ZMB\n",
      "309284 2022-12-31             8.33             13.89      ZMB\n",
      "\n",
      "[20595 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "mask = merged_df['Stringency Index'] != merged_df['stringency_index']\n",
    "\n",
    "# Step 2: Filter the DataFrame using the mask\n",
    "differences = merged_df[mask][['date','Stringency Index','stringency_index','iso_code']].dropna()\n",
    "\n",
    "# test = differences[['stringency_index', 'Stringency Index']].mean(axis=1)\n",
    "# print(test)\n",
    "\n",
    "# Display the rows where the values differ\n",
    "print(\"Rows where Column_A and Column_B differ:\")\n",
    "\n",
    "print(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309280    11.11\n",
      "Name: mean_stringency, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate the mean and add it back to the original DataFrame\n",
    "merged_df['mean_stringency'] = merged_df[['Stringency Index', 'stringency_index']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Step 2: Apply the filtering conditions on the original DataFrame\n",
    "selected_row_mean_check = merged_df.loc[(merged_df['date'] == '2022-12-27') & (merged_df['iso_code'] == 'ZMB'), 'mean_stringency']\n",
    "\n",
    "# Display the result\n",
    "print(selected_row_mean_check)\n",
    "\n",
    "\n",
    "merged_df = merged_df.drop(columns=['Stringency Index'])\n",
    "merged_df = merged_df.drop(columns=['stringency_index'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above block takes the average of the two columns for some row. We can check it is working by computing the average by hand for some row and checking it is the same provided above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us move the continent and location columns to the front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>mean_stringency</th>\n",
       "      <th>CH Index</th>\n",
       "      <th>Gov Resp Index</th>\n",
       "      <th>Econ Sup Index</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>...</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>population</th>\n",
       "      <th>excess_mortality_cumulative_absolute</th>\n",
       "      <th>excess_mortality_cumulative</th>\n",
       "      <th>excess_mortality</th>\n",
       "      <th>excess_mortality_cumulative_per_million</th>\n",
       "      <th>mean_stringency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date iso_code      continent location  mean_stringency CH Index  \\\n",
       "0 2020-01-05      ABW  North America    Aruba              0.0      0.0   \n",
       "1 2020-01-06      ABW  North America    Aruba              0.0      0.0   \n",
       "2 2020-01-07      ABW  North America    Aruba              0.0      0.0   \n",
       "3 2020-01-08      ABW  North America    Aruba              0.0      0.0   \n",
       "4 2020-01-09      ABW  North America    Aruba              0.0      0.0   \n",
       "\n",
       "  Gov Resp Index Econ Sup Index  total_cases  new_cases  ...  \\\n",
       "0            0.0            0.0          0.0        0.0  ...   \n",
       "1            0.0            0.0          0.0        0.0  ...   \n",
       "2            0.0            0.0          0.0        0.0  ...   \n",
       "3            0.0            0.0          0.0        0.0  ...   \n",
       "4            0.0            0.0          0.0        0.0  ...   \n",
       "\n",
       "   handwashing_facilities  hospital_beds_per_thousand  life_expectancy  \\\n",
       "0                     NaN                         NaN            76.29   \n",
       "1                     NaN                         NaN            76.29   \n",
       "2                     NaN                         NaN            76.29   \n",
       "3                     NaN                         NaN            76.29   \n",
       "4                     NaN                         NaN            76.29   \n",
       "\n",
       "   human_development_index  population  excess_mortality_cumulative_absolute  \\\n",
       "0                      NaN      106459                                   NaN   \n",
       "1                      NaN      106459                                   NaN   \n",
       "2                      NaN      106459                                   NaN   \n",
       "3                      NaN      106459                                   NaN   \n",
       "4                      NaN      106459                                   NaN   \n",
       "\n",
       "   excess_mortality_cumulative  excess_mortality  \\\n",
       "0                          NaN               NaN   \n",
       "1                          NaN               NaN   \n",
       "2                          NaN               NaN   \n",
       "3                          NaN               NaN   \n",
       "4                          NaN               NaN   \n",
       "\n",
       "   excess_mortality_cumulative_per_million  mean_stringency  \n",
       "0                                      NaN              0.0  \n",
       "1                                      NaN              0.0  \n",
       "2                                      NaN              0.0  \n",
       "3                                      NaN              0.0  \n",
       "4                                      NaN              0.0  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns = list(merged_df.columns)\n",
    "# Move 'continent' and 'location' next to 'date' and 'iso_code'\n",
    "new_column_order = ['date', 'iso_code', 'continent', 'location', 'mean_stringency'] + [col for col in all_columns if col not in ['date', 'iso_code', 'continent', 'location']]\n",
    "merged_df = merged_df[new_column_order]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to add an additional column to the dataset. this column will be called days_since. This column will effectively represent the date column in our future regression models however these models do not interpret timestamp objects and we will get an error that looks like this:\n",
    "\n",
    "TypeError: float() argument must be a string or a real number, not 'Timestamp'\n",
    "\n",
    "Hence this new column is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['days_since'] = merged_df.groupby('iso_code')['date'].transform(lambda x: (x - x.min()).dt.days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column will start at 0 (representing the first day of recording, the 5th of January 2020) and end at 1673 (representing the last day of recording, the 4th of August 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'iso_code', 'continent', 'location', 'mean_stringency',\n",
      "       'CH Index', 'Gov Resp Index', 'Econ Sup Index', 'total_cases',\n",
      "       'new_cases', 'new_cases_smoothed', 'total_deaths', 'new_deaths',\n",
      "       'new_deaths_smoothed', 'total_cases_per_million',\n",
      "       'new_cases_per_million', 'new_cases_smoothed_per_million',\n",
      "       'total_deaths_per_million', 'new_deaths_per_million',\n",
      "       'new_deaths_smoothed_per_million', 'reproduction_rate', 'icu_patients',\n",
      "       'icu_patients_per_million', 'hosp_patients',\n",
      "       'hosp_patients_per_million', 'weekly_icu_admissions',\n",
      "       'weekly_icu_admissions_per_million', 'weekly_hosp_admissions',\n",
      "       'weekly_hosp_admissions_per_million', 'total_tests', 'new_tests',\n",
      "       'total_tests_per_thousand', 'new_tests_per_thousand',\n",
      "       'new_tests_smoothed', 'new_tests_smoothed_per_thousand',\n",
      "       'positive_rate', 'tests_per_case', 'tests_units', 'total_vaccinations',\n",
      "       'people_vaccinated', 'people_fully_vaccinated', 'total_boosters',\n",
      "       'new_vaccinations', 'new_vaccinations_smoothed',\n",
      "       'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred',\n",
      "       'people_fully_vaccinated_per_hundred', 'total_boosters_per_hundred',\n",
      "       'new_vaccinations_smoothed_per_million',\n",
      "       'new_people_vaccinated_smoothed',\n",
      "       'new_people_vaccinated_smoothed_per_hundred', 'population_density',\n",
      "       'median_age', 'aged_65_older', 'aged_70_older', 'gdp_per_capita',\n",
      "       'extreme_poverty', 'cardiovasc_death_rate', 'diabetes_prevalence',\n",
      "       'female_smokers', 'male_smokers', 'handwashing_facilities',\n",
      "       'hospital_beds_per_thousand', 'life_expectancy',\n",
      "       'human_development_index', 'population',\n",
      "       'excess_mortality_cumulative_absolute', 'excess_mortality_cumulative',\n",
      "       'excess_mortality', 'excess_mortality_cumulative_per_million',\n",
      "       'mean_stringency', 'days_since'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Splitting our Data into Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to split our data up into test and training data. Due to the nature of this particular project where we are aiming to predict the reproduction rate for a particular country over the entire period of time, it makes intuitive sense to due a train/test split on a country level i.e. we aren't going to design a split such that some of a country's data is in training and some of it is in testing. \n",
    "\n",
    "Let us first see how many countries there are in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABW' 'AFG' 'AGO' 'ALB' 'AND' 'ARE' 'ARG' 'AUS' 'AUT' 'AZE' 'BDI' 'BEL'\n",
      " 'BEN' 'BFA' 'BGD' 'BGR' 'BHR' 'BHS' 'BIH' 'BLR' 'BLZ' 'BMU' 'BOL' 'BRA'\n",
      " 'BRB' 'BRN' 'BTN' 'BWA' 'CAF' 'CAN' 'CHE' 'CHL' 'CHN' 'CIV' 'CMR' 'COD'\n",
      " 'COG' 'COL' 'COM' 'CPV' 'CRI' 'CUB' 'CYP' 'CZE' 'DEU' 'DJI' 'DMA' 'DNK'\n",
      " 'DOM' 'DZA' 'ECU' 'EGY' 'ERI' 'ESP' 'EST' 'ETH' 'FIN' 'FJI' 'FRA' 'FRO'\n",
      " 'GAB' 'GBR' 'GEO' 'GHA' 'GIN' 'GMB' 'GRC' 'GRD' 'GRL' 'GTM' 'GUM' 'GUY'\n",
      " 'HKG' 'HND' 'HRV' 'HTI' 'HUN' 'IDN' 'IND' 'IRL' 'IRN' 'IRQ' 'ISL' 'ISR'\n",
      " 'ITA' 'JAM' 'JOR' 'JPN' 'KAZ' 'KEN' 'KGZ' 'KHM' 'KIR' 'KOR' 'KWT' 'LAO'\n",
      " 'LBN' 'LBR' 'LBY' 'LIE' 'LKA' 'LSO' 'LTU' 'LUX' 'LVA' 'MAC' 'MAR' 'MCO'\n",
      " 'MDA' 'MDG' 'MEX' 'MLI' 'MLT' 'MMR' 'MNG' 'MOZ' 'MRT' 'MUS' 'MWI' 'MYS'\n",
      " 'NAM' 'NER' 'NGA' 'NIC' 'NLD' 'NOR' 'NPL' 'NZL' 'OMN' 'PAK' 'PAN' 'PER'\n",
      " 'PHL' 'PNG' 'POL' 'PRI' 'PRT' 'PRY' 'PSE' 'QAT' 'ROU' 'RUS' 'RWA' 'SAU'\n",
      " 'SDN' 'SEN' 'SGP' 'SLB' 'SLE' 'SLV' 'SMR' 'SOM' 'SRB' 'SSD' 'SUR' 'SVK'\n",
      " 'SVN' 'SWE' 'SWZ' 'SYC' 'SYR' 'TCD' 'TGO' 'THA' 'TJK' 'TKM' 'TLS' 'TON'\n",
      " 'TTO' 'TUN' 'TUR' 'TWN' 'TZA' 'UGA' 'UKR' 'URY' 'USA' 'UZB' 'VEN' 'VIR'\n",
      " 'VNM' 'VUT' 'YEM' 'ZAF' 'ZMB' 'ZWE']\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['iso_code'].unique())\n",
    "print(len(merged_df['iso_code'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to have roughly an 80/20 split of training and testing. However, there are 186 countries so in this particular case, we shall randomely select 155 countries to be trained with and the remaining 31 to be tested with. This equates roughly to a 83/17 split of training and testing.\n",
    "\n",
    "To select a random sample of 155 elements from a set of 186 unique ISO codes in a Pandas DataFrame, we can use the sample() method. This method allows you to specify the number of samples you want to draw and will randomly select from the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BFA', 'BLR', 'TUN', 'PRI', 'GUM', 'AZE', 'RUS', 'LTU', 'ERI', 'AGO', 'NLD', 'SUR', 'KAZ', 'BMU', 'MAR', 'TCD', 'MRT', 'BHS', 'BLZ', 'DJI', 'UZB', 'MNG', 'TUR', 'MDA', 'RWA', 'CYP', 'LSO', 'ARG', 'BOL', 'GAB', 'CRI', 'SVK', 'ISR', 'SVN', 'SLV', 'BTN', 'VUT', 'MMR', 'KHM', 'SWZ', 'COL', 'TJK', 'TTO', 'PSE', 'BRA', 'SWE', 'SRB', 'DEU', 'ROU', 'VEN', 'NOR', 'LVA', 'AUS', 'TWN', 'JPN', 'VNM', 'SMR', 'EST', 'JAM', 'USA', 'GEO', 'HTI', 'BGD', 'SAU', 'ZWE', 'ABW', 'SEN', 'MOZ', 'MLI', 'SSD', 'GRL', 'BDI', 'SYR', 'NZL', 'URY', 'LAO', 'CUB', 'KWT', 'SGP', 'DMA', 'COM', 'ALB', 'CAN', 'HND', 'LBR', 'COG', 'ETH', 'IDN', 'VIR', 'BEL', 'PAN', 'DZA', 'MAC', 'IRL', 'GMB', 'PAK', 'GRD', 'IRQ', 'KEN', 'KOR', 'HRV', 'JOR', 'QAT', 'CHL', 'CHN', 'PRT', 'BWA', 'SOM', 'EGY', 'AFG', 'LIE', 'BHR', 'FJI', 'SLB', 'TKM', 'CMR', 'ZMB', 'LBN', 'MUS', 'NIC', 'TLS', 'CZE', 'TGO', 'CPV', 'DNK', 'IND', 'KIR', 'PER', 'GIN', 'MCO', 'NPL', 'BRN', 'BRB', 'LUX', 'FRA', 'AUT', 'CIV', 'GUY', 'NER', 'GRC', 'GHA', 'SYC', 'UKR', 'SDN', 'CHE', 'HUN', 'PHL', 'MWI', 'GTM', 'TON', 'OMN', 'MDG', 'PRY', 'CAF', 'KGZ']\n",
      "155\n",
      "['ISL', 'LBY', 'NGA', 'PNG', 'LKA', 'DOM', 'IRN', 'TZA', 'GBR', 'BIH', 'COD', 'BGR', 'AND', 'ECU', 'FRO', 'FIN', 'ESP', 'SLE', 'MYS', 'MEX', 'POL', 'ARE', 'NAM', 'MLT', 'HKG', 'ITA', 'THA', 'YEM', 'BEN', 'UGA', 'ZAF']\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(21) # for replicability, let's introduce a random seed\n",
    "unique_iso_codes = merged_df['iso_code'].unique()\n",
    "random_sample = np.random.choice(unique_iso_codes, size=155, replace=False)\n",
    "# Convert to list for easier handling if needed\n",
    "random_sample_list = random_sample.tolist()\n",
    "# Display the randomly selected iso codes\n",
    "print(random_sample_list)\n",
    "print(len(random_sample_list))\n",
    "\n",
    "training_iso_codes = random_sample_list\n",
    "testing_iso_codes = list(set(unique_iso_codes) - set(training_iso_codes))\n",
    "\n",
    "print(testing_iso_codes)\n",
    "print(len(testing_iso_codes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we've got our training and testing split. Now let's split the dataset itself into our training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m[merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso_code\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(training_iso_codes)]\n\u001b[0;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m merged_df[merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso_code\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(testing_iso_codes)]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = merged_df[merged_df['iso_code'].isin(training_iso_codes)]\n",
    "test_data = merged_df[merged_df['iso_code'].isin(testing_iso_codes)]\n",
    "\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have succefully split our data into training data and test date, augmented with the indices data from the Oxford Dataset. the following code cell takes approximately 10 minutes to run. For that reason, instead of running it yourself, the output train and test xlsx files can be accessed directly via the github in Train_and_Test_data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_excel('train.xlsx', index=False)\n",
    "test_data.to_excel('test.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALT APPROACH:\n",
    "\n",
    "splitting dataset by a certain date instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-26 19:12:00\n",
      "             date iso_code      continent  location  mean_stringency CH Index  \\\n",
      "0      2020-01-05      ABW  North America     Aruba              0.0      0.0   \n",
      "1      2020-01-06      ABW  North America     Aruba              0.0      0.0   \n",
      "2      2020-01-07      ABW  North America     Aruba              0.0      0.0   \n",
      "3      2020-01-08      ABW  North America     Aruba              0.0      0.0   \n",
      "4      2020-01-09      ABW  North America     Aruba              0.0      0.0   \n",
      "...           ...      ...            ...       ...              ...      ...   \n",
      "310735 2022-05-22      ZWE         Africa  Zimbabwe            47.13    53.52   \n",
      "310736 2022-05-23      ZWE         Africa  Zimbabwe            47.13    53.52   \n",
      "310737 2022-05-24      ZWE         Africa  Zimbabwe            47.13    53.52   \n",
      "310738 2022-05-25      ZWE         Africa  Zimbabwe            47.13    53.52   \n",
      "310739 2022-05-26      ZWE         Africa  Zimbabwe            47.13    53.51   \n",
      "\n",
      "       Gov Resp Index Econ Sup Index  total_cases  new_cases  ...  \\\n",
      "0                 0.0            0.0          0.0        0.0  ...   \n",
      "1                 0.0            0.0          0.0        0.0  ...   \n",
      "2                 0.0            0.0          0.0        0.0  ...   \n",
      "3                 0.0            0.0          0.0        0.0  ...   \n",
      "4                 0.0            0.0          0.0        0.0  ...   \n",
      "...               ...            ...          ...        ...  ...   \n",
      "310735          46.83            0.0     250642.0     1436.0  ...   \n",
      "310736          46.83            0.0     250642.0        0.0  ...   \n",
      "310737          46.83            0.0     250642.0        0.0  ...   \n",
      "310738          46.83            0.0     250642.0        0.0  ...   \n",
      "310739          46.82            0.0     250642.0        0.0  ...   \n",
      "\n",
      "        hospital_beds_per_thousand  life_expectancy  human_development_index  \\\n",
      "0                              NaN            76.29                      NaN   \n",
      "1                              NaN            76.29                      NaN   \n",
      "2                              NaN            76.29                      NaN   \n",
      "3                              NaN            76.29                      NaN   \n",
      "4                              NaN            76.29                      NaN   \n",
      "...                            ...              ...                      ...   \n",
      "310735                         1.7            61.49                    0.571   \n",
      "310736                         1.7            61.49                    0.571   \n",
      "310737                         1.7            61.49                    0.571   \n",
      "310738                         1.7            61.49                    0.571   \n",
      "310739                         1.7            61.49                    0.571   \n",
      "\n",
      "        population  excess_mortality_cumulative_absolute  \\\n",
      "0           106459                                   NaN   \n",
      "1           106459                                   NaN   \n",
      "2           106459                                   NaN   \n",
      "3           106459                                   NaN   \n",
      "4           106459                                   NaN   \n",
      "...            ...                                   ...   \n",
      "310735    16320539                                   NaN   \n",
      "310736    16320539                                   NaN   \n",
      "310737    16320539                                   NaN   \n",
      "310738    16320539                                   NaN   \n",
      "310739    16320539                                   NaN   \n",
      "\n",
      "        excess_mortality_cumulative  excess_mortality  \\\n",
      "0                               NaN               NaN   \n",
      "1                               NaN               NaN   \n",
      "2                               NaN               NaN   \n",
      "3                               NaN               NaN   \n",
      "4                               NaN               NaN   \n",
      "...                             ...               ...   \n",
      "310735                          NaN               NaN   \n",
      "310736                          NaN               NaN   \n",
      "310737                          NaN               NaN   \n",
      "310738                          NaN               NaN   \n",
      "310739                          NaN               NaN   \n",
      "\n",
      "        excess_mortality_cumulative_per_million  mean_stringency  days_since  \n",
      "0                                           NaN              0.0           0  \n",
      "1                                           NaN              0.0           1  \n",
      "2                                           NaN              0.0           2  \n",
      "3                                           NaN              0.0           3  \n",
      "4                                           NaN              0.0           4  \n",
      "...                                         ...              ...         ...  \n",
      "310735                                      NaN            47.13         868  \n",
      "310736                                      NaN            47.13         869  \n",
      "310737                                      NaN            47.13         870  \n",
      "310738                                      NaN            47.13         871  \n",
      "310739                                      NaN            47.13         872  \n",
      "\n",
      "[162804 rows x 72 columns]\n",
      "             date iso_code      continent  location  mean_stringency CH Index  \\\n",
      "873    2022-05-27      ABW  North America     Aruba            25.93    33.33   \n",
      "874    2022-05-28      ABW  North America     Aruba            25.93    33.33   \n",
      "875    2022-05-29      ABW  North America     Aruba            25.93    33.33   \n",
      "876    2022-05-30      ABW  North America     Aruba            25.93    33.33   \n",
      "877    2022-05-31      ABW  North America     Aruba            25.93    33.33   \n",
      "...           ...      ...            ...       ...              ...      ...   \n",
      "311536 2024-07-31      ZWE         Africa  Zimbabwe              NaN      NaN   \n",
      "311537 2024-08-01      ZWE         Africa  Zimbabwe              NaN      NaN   \n",
      "311538 2024-08-02      ZWE         Africa  Zimbabwe              NaN      NaN   \n",
      "311539 2024-08-03      ZWE         Africa  Zimbabwe              NaN      NaN   \n",
      "311540 2024-08-04      ZWE         Africa  Zimbabwe              NaN      NaN   \n",
      "\n",
      "       Gov Resp Index Econ Sup Index  total_cases  new_cases  ...  \\\n",
      "873             29.17            0.0      35920.0        0.0  ...   \n",
      "874             29.17            0.0      35920.0        0.0  ...   \n",
      "875             29.17            0.0      36564.0      644.0  ...   \n",
      "876             29.17            0.0      36564.0        0.0  ...   \n",
      "877             29.17            0.0      36564.0        0.0  ...   \n",
      "...               ...            ...          ...        ...  ...   \n",
      "311536            NaN            NaN     266386.0        0.0  ...   \n",
      "311537            NaN            NaN     266386.0        0.0  ...   \n",
      "311538            NaN            NaN     266386.0        0.0  ...   \n",
      "311539            NaN            NaN     266386.0        0.0  ...   \n",
      "311540            NaN            NaN     266386.0        0.0  ...   \n",
      "\n",
      "        hospital_beds_per_thousand  life_expectancy  human_development_index  \\\n",
      "873                            NaN            76.29                      NaN   \n",
      "874                            NaN            76.29                      NaN   \n",
      "875                            NaN            76.29                      NaN   \n",
      "876                            NaN            76.29                      NaN   \n",
      "877                            NaN            76.29                      NaN   \n",
      "...                            ...              ...                      ...   \n",
      "311536                         1.7            61.49                    0.571   \n",
      "311537                         1.7            61.49                    0.571   \n",
      "311538                         1.7            61.49                    0.571   \n",
      "311539                         1.7            61.49                    0.571   \n",
      "311540                         1.7            61.49                    0.571   \n",
      "\n",
      "        population  excess_mortality_cumulative_absolute  \\\n",
      "873         106459                                   NaN   \n",
      "874         106459                                   NaN   \n",
      "875         106459                                   NaN   \n",
      "876         106459                                   NaN   \n",
      "877         106459                                   NaN   \n",
      "...            ...                                   ...   \n",
      "311536    16320539                                   NaN   \n",
      "311537    16320539                                   NaN   \n",
      "311538    16320539                                   NaN   \n",
      "311539    16320539                                   NaN   \n",
      "311540    16320539                                   NaN   \n",
      "\n",
      "        excess_mortality_cumulative  excess_mortality  \\\n",
      "873                             NaN               NaN   \n",
      "874                             NaN               NaN   \n",
      "875                             NaN               NaN   \n",
      "876                             NaN               NaN   \n",
      "877                             NaN               NaN   \n",
      "...                             ...               ...   \n",
      "311536                          NaN               NaN   \n",
      "311537                          NaN               NaN   \n",
      "311538                          NaN               NaN   \n",
      "311539                          NaN               NaN   \n",
      "311540                          NaN               NaN   \n",
      "\n",
      "        excess_mortality_cumulative_per_million  mean_stringency  days_since  \n",
      "873                                         NaN            25.93         873  \n",
      "874                                         NaN            25.93         874  \n",
      "875                                         NaN            25.93         875  \n",
      "876                                         NaN            25.93         876  \n",
      "877                                         NaN            25.93         877  \n",
      "...                                         ...              ...         ...  \n",
      "311536                                      NaN              NaN        1669  \n",
      "311537                                      NaN              NaN        1670  \n",
      "311538                                      NaN              NaN        1671  \n",
      "311539                                      NaN              NaN        1672  \n",
      "311540                                      NaN              NaN        1673  \n",
      "\n",
      "[148737 rows x 72 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the start and end dates for splitting\n",
    "\n",
    "# Define the start and end dates for splitting\n",
    "\n",
    "\n",
    "\n",
    "start_date = pd.to_datetime('2020-01-05')\n",
    "end_date = pd.to_datetime('2022-12-31')\n",
    "\n",
    "# Calculate the 80% cutoff date\n",
    "total_duration = end_date - start_date  # Total duration as a Timedelta\n",
    "cutoff_date = start_date + (total_duration * 0.8)  # Calculate cutoff date\n",
    "\n",
    "print(cutoff_date)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_set2 = merged_df[merged_df['date'] <= cutoff_date]\n",
    "test_set2 = merged_df[merged_df['date'] > cutoff_date]\n",
    "\n",
    "print(train_set2)\n",
    "print(test_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mean_stringency', 'CH Index', 'Gov Resp Index', 'Econ Sup Index',\n",
      "       'total_cases', 'new_cases', 'new_cases_smoothed', 'total_deaths',\n",
      "       'new_deaths', 'new_deaths_smoothed', 'total_cases_per_million',\n",
      "       'new_cases_per_million', 'new_cases_smoothed_per_million',\n",
      "       'total_deaths_per_million', 'new_deaths_per_million',\n",
      "       'new_deaths_smoothed_per_million', 'reproduction_rate', 'icu_patients',\n",
      "       'icu_patients_per_million', 'hosp_patients',\n",
      "       'hosp_patients_per_million', 'weekly_icu_admissions',\n",
      "       'weekly_icu_admissions_per_million', 'weekly_hosp_admissions',\n",
      "       'weekly_hosp_admissions_per_million', 'total_tests', 'new_tests',\n",
      "       'total_tests_per_thousand', 'new_tests_per_thousand',\n",
      "       'new_tests_smoothed', 'new_tests_smoothed_per_thousand',\n",
      "       'positive_rate', 'tests_per_case', 'tests_units', 'total_vaccinations',\n",
      "       'people_vaccinated', 'people_fully_vaccinated', 'total_boosters',\n",
      "       'new_vaccinations', 'new_vaccinations_smoothed',\n",
      "       'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred',\n",
      "       'people_fully_vaccinated_per_hundred', 'total_boosters_per_hundred',\n",
      "       'new_vaccinations_smoothed_per_million',\n",
      "       'new_people_vaccinated_smoothed',\n",
      "       'new_people_vaccinated_smoothed_per_hundred', 'population_density',\n",
      "       'median_age', 'aged_65_older', 'aged_70_older', 'gdp_per_capita',\n",
      "       'extreme_poverty', 'cardiovasc_death_rate', 'diabetes_prevalence',\n",
      "       'female_smokers', 'male_smokers', 'handwashing_facilities',\n",
      "       'hospital_beds_per_thousand', 'life_expectancy',\n",
      "       'human_development_index', 'population',\n",
      "       'excess_mortality_cumulative_absolute', 'excess_mortality_cumulative',\n",
      "       'excess_mortality', 'excess_mortality_cumulative_per_million',\n",
      "       'mean_stringency', 'days_since'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_data_string_cols_rem2 = train_set2.iloc[:, 4:]\n",
    "test_data_string_cols_rem2 = test_set2.iloc[:, 4:]\n",
    "\n",
    "print(train_data_string_cols_rem2.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(130874, 6)\n",
      "130874\n",
      "       mean_stringency mean_stringency CH Index Gov Resp Index Econ Sup Index  \\\n",
      "2547             11.11           11.11    18.21          15.94            0.0   \n",
      "2548             11.11           11.11    18.21          15.94            0.0   \n",
      "2549             11.11           11.11    18.21          15.94            0.0   \n",
      "2550             11.11           11.11    18.21          15.94            0.0   \n",
      "2551             11.11           11.11    18.21          15.94            0.0   \n",
      "...                ...             ...      ...            ...            ...   \n",
      "310954            53.7            53.7    57.96          50.72            0.0   \n",
      "310955            53.7            53.7    57.96          50.72            0.0   \n",
      "310956            53.7            53.7    57.96          50.72            0.0   \n",
      "310957            53.7            53.7    57.96          50.72            0.0   \n",
      "310958            53.7            53.7    57.96          50.72            0.0   \n",
      "\n",
      "        days_since  \n",
      "2547           873  \n",
      "2548           874  \n",
      "2549           875  \n",
      "2550           876  \n",
      "2551           877  \n",
      "...            ...  \n",
      "310954        1087  \n",
      "310955        1088  \n",
      "310956        1089  \n",
      "310957        1090  \n",
      "310958        1091  \n",
      "\n",
      "[37887 rows x 6 columns]\n",
      "\n",
      "(37887, 6)\n",
      "37887\n"
     ]
    }
   ],
   "source": [
    "covariates = ['mean_stringency','CH Index', 'Gov Resp Index', 'Econ Sup Index','days_since']\n",
    "covariates_and_repr = covariates + ['reproduction_rate']\n",
    "\n",
    "train_data_filtered = train_data_string_cols_rem2[covariates_and_repr].dropna()\n",
    "\n",
    "X_train = train_data_filtered[covariates]\n",
    "y_train = train_data_filtered[['reproduction_rate']]\n",
    "\n",
    "print()\n",
    "print(X_train.shape)\n",
    "print(len(y_train))\n",
    "\n",
    "test_data_filtered = test_data_string_cols_rem2[covariates_and_repr].dropna()\n",
    "\n",
    "X_test = test_data_filtered[covariates]\n",
    "y_test = test_data_filtered[['reproduction_rate']]\n",
    "\n",
    "print(X_test)\n",
    "\n",
    "\n",
    "print()\n",
    "print(X_test.shape)\n",
    "print(len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean_stringency mean_stringency CH Index Gov Resp Index Econ Sup Index  \\\n",
      "2547             11.11           11.11    18.21          15.94            0.0   \n",
      "2548             11.11           11.11    18.21          15.94            0.0   \n",
      "2549             11.11           11.11    18.21          15.94            0.0   \n",
      "2550             11.11           11.11    18.21          15.94            0.0   \n",
      "2551             11.11           11.11    18.21          15.94            0.0   \n",
      "...                ...             ...      ...            ...            ...   \n",
      "310954            53.7            53.7    57.96          50.72            0.0   \n",
      "310955            53.7            53.7    57.96          50.72            0.0   \n",
      "310956            53.7            53.7    57.96          50.72            0.0   \n",
      "310957            53.7            53.7    57.96          50.72            0.0   \n",
      "310958            53.7            53.7    57.96          50.72            0.0   \n",
      "\n",
      "        days_since  \n",
      "2547           873  \n",
      "2548           874  \n",
      "2549           875  \n",
      "2550           876  \n",
      "2551           877  \n",
      "...            ...  \n",
      "310954        1087  \n",
      "310955        1088  \n",
      "310956        1089  \n",
      "310957        1090  \n",
      "310958        1091  \n",
      "\n",
      "[37887 rows x 6 columns]\n",
      "y_pred shape: (37887, 1)\n",
      "Mean Squared Error on the test data: 0.1914\n",
      "Mean Absolute Error on the test data: 0.3588\n",
      "R^2: 0.0387\n"
     ]
    }
   ],
   "source": [
    "# Initialize the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "print(X_test)\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 100 / (1 + np.exp(-x))  # Scaling sigmoid to range 0-100\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = linear_model.predict(X_test)\n",
    "# y_pred_sigmoid = sigmoid(y_pred)\n",
    "print(\"y_pred shape:\", y_pred.shape)\n",
    "# Calculate Mean Squared Error (MSE) on the test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "R_squared = linear_model.score(X_test, y_test)\n",
    "\n",
    "# Print the MSE result\n",
    "print(f'Mean Squared Error on the test data: {mse:.4f}')\n",
    "print(f'Mean Absolute Error on the test data: {mae:.4f}')\n",
    "print(f'R^2: {R_squared:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
